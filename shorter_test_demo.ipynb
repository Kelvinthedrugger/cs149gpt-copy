{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### source\n",
        "\n",
        "result tested on this [commit](https://github.com/Kelvinthedrugger/cs149gpt-copy/tree/a2ad3f6764c5ba1c9210a9e3353380f143f7c544) (branch: [openmp](https://github.com/Kelvinthedrugger/cs149gpt-copy/tree/openmp))\n",
        "\n",
        "\n",
        "one can use the original version of gpt149.py from [this commit](https://github.com/Kelvinthedrugger/cs149gpt-copy/commit/d1ae74d12e8e5f7a90ec276cc69ffec07b34861f)\n",
        "\n",
        "\n",
        "the branch that contains the rewritten version of gpt149.py is [here](https://github.com/Kelvinthedrugger/cs149gpt-copy/tree/shorter_test)"
      ],
      "metadata": {
        "id": "6g7jMKZCcsOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --single-branch --branch openmp https://github.com/Kelvinthedrugger/cs149gpt-copy.git"
      ],
      "metadata": {
        "id": "pNkMSe8va90Q",
        "outputId": "26f907dc-4103-4d78-dfb5-350b73576ad8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cs149gpt-copy'...\n",
            "remote: Enumerating objects: 192, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 192 (delta 78), reused 74 (delta 65), pack-reused 100 (from 1)\u001b[K\n",
            "Receiving objects: 100% (192/192), 11.98 MiB | 7.52 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd cs149gpt-copy"
      ],
      "metadata": {
        "id": "FkRldZZmbAYo",
        "outputId": "79eaf283-2faa-40b3-8abd-e0cde897eac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cs149gpt-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch"
      ],
      "metadata": {
        "id": "h39RFwf9bDmx",
        "outputId": "0564d2fc-62b8-430b-e499-07eab87aac9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mopenmp\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "need torch==2.1.2 to use .so file"
      ],
      "metadata": {
        "id": "nlA9Y2SIxlci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat requirements.txt"
      ],
      "metadata": {
        "id": "KHVAIKDtbEci",
        "outputId": "eb6f2db4-06db-47ea-e0db-60cea45d1c57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch==2.1.2 --index-url https://download.pytorch.org/whl/cpu # to run module_ref.so on x86-64\n",
            "Ninja\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "error from pip's dependency resolver won't affect our test result"
      ],
      "metadata": {
        "id": "EiggBgNDxttL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6jY2xpSxbHhY",
        "outputId": "1b7541aa-d2bf-47d6-d13b-d2a72c8052af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch"
      ],
      "metadata": {
        "id": "OykpLNybccfL",
        "outputId": "f7313331-afb9-4833-ac1c-534c8b6ce120",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mopenmp\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result run with the original gpt149.py"
      ],
      "metadata": {
        "id": "sm9fALd6wmfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to run with the original gpt149.py, uncomment & execute this block\n",
        "\n",
        "\n",
        "optional, since i did very little modification in branch openmp"
      ],
      "metadata": {
        "id": "hipgk-g-yRwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm gpt149.py\n",
        "# url_o = \"https://raw.githubusercontent.com/Kelvinthedrugger/cs149gpt-copy/d1ae74d12e8e5f7a90ec276cc69ffec07b34861f/gpt149.py\"\n",
        "# !wget {url_o}"
      ],
      "metadata": {
        "id": "tKzEGGwbxzed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part1"
      ],
      "metadata": {
        "id": "DV97JMsCbK1C",
        "outputId": "acd0d1cc-f99a-46be-e5e4-c27176a6803d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "Running Part 1 Test: Naive Unfused Attention\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 11:14:49 1258:1258 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 11:14:50 1258:1258 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 11:14:50 1258:1258 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.3505535125732422 \n",
            "\n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                    aten::empty         0.02%      82.000us         0.02%      82.000us      27.333us       5.00 Mb       5.00 Mb             3  \n",
            "    REFERENCE - NAIVE ATTENTION        99.00%     347.090ms        99.98%     350.515ms     350.515ms       4.50 Mb      -1.00 Mb             1  \n",
            "                    aten::zeros         0.02%      62.000us         0.63%       2.223ms       1.111ms       4.50 Mb           0 b             2  \n",
            "                    aten::clone         0.03%     107.000us         0.31%       1.093ms     546.500us       1.00 Mb           0 b             2  \n",
            "                model_inference         0.02%      82.000us       100.00%     350.597ms     350.597ms     512.00 Kb      -4.00 Mb             1  \n",
            "                  aten::flatten         0.02%      79.000us         0.21%     727.000us     145.400us     512.00 Kb           0 b             5  \n",
            "               aten::empty_like         0.00%      14.000us         0.01%      32.000us      32.000us     512.00 Kb           0 b             1  \n",
            "            aten::empty_strided         0.01%      41.000us         0.01%      41.000us      41.000us     512.00 Kb     512.00 Kb             1  \n",
            "                    aten::zero_         0.03%      99.000us         0.60%       2.097ms       1.048ms           0 b           0 b             2  \n",
            "                    aten::fill_         0.57%       1.998ms         0.57%       1.998ms     999.000us           0 b           0 b             2  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 350.597ms\n",
            "\n",
            "REFERENCE - NAIVE ATTENTION statistics\n",
            "cpu time:  350.515ms\n",
            "mem usage:  4718592 bytes\n",
            "ERROR:2024-11-02 11:14:50 1258:1258 CudaDeviceProperties.cpp:27] cudaGetDeviceCount failed with code 35\n",
            "dump REFERENCE_-_NAIVE_ATTENTION.json succeeded\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 11:14:57 1258:1258 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 11:14:58 1258:1258 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 11:14:58 1258:1258 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.3228936195373535 \n",
            "\n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  aten::empty         0.01%      43.000us         0.01%      43.000us      14.333us       5.00 Mb       5.00 Mb             3  \n",
            "    STUDENT - NAIVE ATTENTION        99.54%     321.446ms        99.97%     322.849ms     322.849ms       4.50 Mb      -1.00 Mb             1  \n",
            "                  aten::zeros         0.01%      33.000us         0.21%     678.000us     339.000us       4.50 Mb           0 b             2  \n",
            "                  aten::clone         0.01%      48.000us         0.20%     657.000us     328.500us       1.00 Mb           0 b             2  \n",
            "              model_inference         0.03%      89.000us       100.00%     322.938ms     322.938ms     512.00 Kb      -4.00 Mb             1  \n",
            "                aten::flatten         0.01%      43.000us         0.12%     402.000us      80.400us     512.00 Kb           0 b             5  \n",
            "             aten::empty_like         0.00%       7.000us         0.00%      14.000us      14.000us     512.00 Kb           0 b             1  \n",
            "          aten::empty_strided         0.00%      16.000us         0.00%      16.000us      16.000us     512.00 Kb     512.00 Kb             1  \n",
            "                  aten::zero_         0.01%      22.000us         0.19%     609.000us     304.500us           0 b           0 b             2  \n",
            "                  aten::fill_         0.18%     587.000us         0.18%     587.000us     293.500us           0 b           0 b             2  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 322.938ms\n",
            "\n",
            "STUDENT - NAIVE ATTENTION statistics\n",
            "cpu time:  322.849ms\n",
            "mem usage:  4718592 bytes\n",
            "dump STUDENT_-_NAIVE_ATTENTION.json succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part2"
      ],
      "metadata": {
        "id": "JFoRcJnzbU75",
        "outputId": "46a510a2-b81c-43ef-c670-957999779167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "Running Part 2 Test: Unfused Attention with Blocked Matmul\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 11:15:14 1557:1557 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 11:15:14 1557:1557 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 11:15:14 1557:1557 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.28053903579711914 \n",
            "\n",
            "------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                     aten::empty         0.02%      66.000us         0.02%      66.000us      22.000us       5.00 Mb       5.00 Mb             3  \n",
            "    REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX        98.82%     277.257ms        99.96%     280.470ms     280.470ms       4.50 Mb      -1.00 Mb             1  \n",
            "                                     aten::zeros         0.03%      83.000us         0.72%       2.018ms       1.009ms       4.50 Mb           0 b             2  \n",
            "                                     aten::clone         0.03%      88.000us         0.39%       1.091ms     545.500us       1.00 Mb           0 b             2  \n",
            "                                 model_inference         0.04%     109.000us       100.00%     280.579ms     280.579ms     512.00 Kb      -4.00 Mb             1  \n",
            "                                   aten::flatten         0.03%      72.000us         0.25%     700.000us     140.000us     512.00 Kb           0 b             5  \n",
            "                                aten::empty_like         0.00%      13.000us         0.01%      31.000us      31.000us     512.00 Kb           0 b             1  \n",
            "                             aten::empty_strided         0.01%      32.000us         0.01%      32.000us      32.000us     512.00 Kb     512.00 Kb             1  \n",
            "                                     aten::zero_         0.02%      53.000us         0.67%       1.887ms     943.500us           0 b           0 b             2  \n",
            "                                     aten::fill_         0.65%       1.834ms         0.65%       1.834ms     917.000us           0 b           0 b             2  \n",
            "------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 280.579ms\n",
            "\n",
            "REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics\n",
            "cpu time:  280.47ms\n",
            "mem usage:  4718592 bytes\n",
            "ERROR:2024-11-02 11:15:14 1557:1557 CudaDeviceProperties.cpp:27] cudaGetDeviceCount failed with code 35\n",
            "dump REFERENCE_-_BLOCKED_MATMUL_+_UNFUSED_SOFTMAX.json succeeded\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 11:15:23 1557:1557 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 11:15:24 1557:1557 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 11:15:24 1557:1557 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  1.1246929168701172 \n",
            "\n",
            "----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                   aten::empty         0.00%      56.000us         0.00%      56.000us      18.667us       5.00 Mb       5.00 Mb             3  \n",
            "    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.81%        1.123s        99.99%        1.125s        1.125s       4.50 Mb      -1.00 Mb             1  \n",
            "                                   aten::zeros         0.00%      36.000us         0.07%     807.000us     403.500us       4.50 Mb           0 b             2  \n",
            "                                   aten::clone         0.00%      54.000us         0.10%       1.149ms     574.500us       1.00 Mb           0 b             2  \n",
            "                               model_inference         0.01%      82.000us       100.00%        1.125s        1.125s     512.00 Kb      -4.00 Mb             1  \n",
            "                                 aten::flatten         0.00%      46.000us         0.07%     734.000us     146.800us     512.00 Kb           0 b             5  \n",
            "                              aten::empty_like         0.00%       8.000us         0.00%      35.000us      35.000us     512.00 Kb           0 b             1  \n",
            "                           aten::empty_strided         0.00%      36.000us         0.00%      36.000us      36.000us     512.00 Kb     512.00 Kb             1  \n",
            "                                   aten::zero_         0.00%      17.000us         0.07%     742.000us     371.000us           0 b           0 b             2  \n",
            "                                   aten::fill_         0.06%     725.000us         0.06%     725.000us     362.500us           0 b           0 b             2  \n",
            "----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.125s\n",
            "\n",
            "STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics\n",
            "cpu time:  1124.652ms\n",
            "mem usage:  4718592 bytes\n",
            "dump STUDENT_-_BLOCKED_MATMUL_+_UNFUSED_SOFTMAX.json succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part3"
      ],
      "metadata": {
        "id": "ZQsqEL3H7jEZ",
        "outputId": "4d52a137-98dd-4012-87e6-b88367c8f6d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "Running Part 3 Test: Fused Attention\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 11:15:38 1700:1700 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 11:15:39 1700:1700 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 11:15:39 1700:1700 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.24706292152404785 \n",
            "\n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                    aten::empty         0.02%      42.000us         0.02%      42.000us      14.000us       1.03 Mb       1.03 Mb             3  \n",
            "                    aten::clone         0.03%      78.000us         0.35%     856.000us     428.000us       1.00 Mb           0 b             2  \n",
            "    REFERENCE - FUSED ATTENTION        97.82%     241.721ms        99.97%     247.027ms     247.027ms     544.00 Kb      -1.00 Mb             1  \n",
            "                    aten::zeros         0.02%      43.000us         0.10%     248.000us     124.000us     544.00 Kb           0 b             2  \n",
            "                model_inference         0.03%      73.000us       100.00%     247.100ms     247.100ms     512.00 Kb     -32.00 Kb             1  \n",
            "                  aten::flatten         0.47%       1.151ms         0.90%       2.220ms       4.302us     512.00 Kb           0 b           516  \n",
            "               aten::empty_like         0.01%      14.000us         0.02%      38.000us      38.000us     512.00 Kb           0 b             1  \n",
            "            aten::empty_strided         0.01%      22.000us         0.01%      22.000us      22.000us     512.00 Kb     512.00 Kb             1  \n",
            "                    aten::zero_         0.02%      38.000us         0.08%     187.000us      93.500us           0 b           0 b             2  \n",
            "                    aten::fill_         0.06%     149.000us         0.06%     149.000us     149.000us           0 b           0 b             1  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 247.100ms\n",
            "\n",
            "REFERENCE - FUSED ATTENTION statistics\n",
            "cpu time:  247.027ms\n",
            "mem usage:  557056 bytes\n",
            "ERROR:2024-11-02 11:15:39 1700:1700 CudaDeviceProperties.cpp:27] cudaGetDeviceCount failed with code 35\n",
            "dump REFERENCE_-_FUSED_ATTENTION.json succeeded\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 11:15:47 1700:1700 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 11:15:47 1700:1700 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 11:15:47 1700:1700 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.258960485458374 \n",
            "\n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  aten::empty         0.01%      28.000us         0.01%      28.000us       7.000us       1.04 Mb       1.04 Mb             4  \n",
            "                  aten::clone         0.01%      36.000us         0.21%     555.000us     277.500us       1.00 Mb           0 b             2  \n",
            "                  aten::zeros         0.01%      34.000us         0.08%     219.000us      73.000us     548.00 Kb           0 b             3  \n",
            "    STUDENT - FUSED ATTENTION        98.02%     253.861ms        99.97%     258.928ms     258.928ms     544.00 Kb      -1.00 Mb             1  \n",
            "              model_inference         0.03%      67.000us       100.00%     258.995ms     258.995ms     512.00 Kb     -32.00 Kb             1  \n",
            "                aten::flatten         0.43%       1.116ms         0.82%       2.118ms       4.105us     512.00 Kb           0 b           516  \n",
            "             aten::empty_like         0.00%       9.000us         0.01%      15.000us      15.000us     512.00 Kb           0 b             1  \n",
            "          aten::empty_strided         0.00%      11.000us         0.00%      11.000us      11.000us     512.00 Kb     512.00 Kb             1  \n",
            "                  aten::zero_         0.01%      17.000us         0.06%     163.000us      54.333us           0 b           0 b             3  \n",
            "                  aten::fill_         0.06%     146.000us         0.06%     146.000us     146.000us           0 b           0 b             1  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 258.995ms\n",
            "\n",
            "STUDENT - FUSED ATTENTION statistics\n",
            "cpu time:  258.928ms\n",
            "mem usage:  557056 bytes\n",
            "dump STUDENT_-_FUSED_ATTENTION.json succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set Bc = Br = 16 to give a better performance\n",
        "!python3 gpt149.py part4 -bc 16 -br 16"
      ],
      "metadata": {
        "id": "7Fi3oJTRbXzH",
        "outputId": "1d16b636-4349-41ea-a70e-fe6429490c42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "Running Part 4 Test: Flash Attention\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 11:16:02 1822:1822 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 11:16:03 1822:1822 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 11:16:03 1822:1822 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.6980502605438232 \n",
            "\n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                    aten::zeros         0.01%      78.000us         0.49%       3.422ms     244.429us       8.52 Mb           0 b            14  \n",
            "                    aten::empty         0.01%      81.000us         0.01%      81.000us       5.786us       8.52 Mb       8.52 Mb            14  \n",
            "                model_inference         0.03%     237.000us       100.00%     698.090ms     698.090ms     512.00 Kb     -16.19 Kb             1  \n",
            "    REFERENCE - FLASH ATTENTION        99.12%     691.967ms        99.95%     697.767ms     697.767ms     512.00 Kb      -8.00 Mb             1  \n",
            "                    aten::zero_         0.36%       2.521ms         0.82%       5.727ms       0.069us           0 b           0 b         82444  \n",
            "                    aten::fill_         0.46%       3.206ms         0.46%       3.206ms       1.069ms           0 b           0 b             3  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 698.090ms\n",
            "\n",
            "REFERENCE - FLASH ATTENTION statistics\n",
            "cpu time:  697.767ms\n",
            "mem usage:  524288 bytes\n",
            "ERROR:2024-11-02 11:16:15 1822:1822 CudaDeviceProperties.cpp:27] cudaGetDeviceCount failed with code 35\n",
            "dump REFERENCE_-_FLASH_ATTENTION.json succeeded\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 11:16:23 1822:1822 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 11:16:24 1822:1822 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 11:16:24 1822:1822 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.27485036849975586 \n",
            "\n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  aten::empty         0.01%      37.000us         0.01%      37.000us       2.846us       1.02 Mb       1.02 Mb            13  \n",
            "                  aten::clone         0.03%      82.000us         0.23%     637.000us     318.500us       1.00 Mb           0 b             2  \n",
            "                  aten::zeros         0.02%      52.000us         0.09%     239.000us      19.917us     528.19 Kb           0 b            12  \n",
            "              model_inference         0.06%     155.000us       100.00%     274.889ms     274.889ms     512.00 Kb     -16.19 Kb             1  \n",
            "    STUDENT - FLASH ATTENTION        99.59%     273.753ms        99.92%     274.659ms     274.659ms     512.00 Kb      -1.00 Mb             1  \n",
            "                aten::flatten         0.03%      76.000us         0.18%     484.000us      32.267us     512.00 Kb           0 b            15  \n",
            "             aten::empty_like         0.00%      12.000us         0.01%      20.000us      20.000us     512.00 Kb           0 b             1  \n",
            "          aten::empty_strided         0.01%      14.000us         0.01%      14.000us      14.000us     512.00 Kb     512.00 Kb             1  \n",
            "                  aten::zero_         0.00%      11.000us         0.06%     158.000us      13.167us           0 b           0 b            12  \n",
            "                  aten::fill_         0.05%     147.000us         0.05%     147.000us     147.000us           0 b           0 b             1  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 274.889ms\n",
            "\n",
            "STUDENT - FLASH ATTENTION statistics\n",
            "cpu time:  274.659ms\n",
            "mem usage:  524288 bytes\n",
            "dump STUDENT_-_FLASH_ATTENTION.json succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgp-2HtRb0hX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the rewritten version of gpt149.py"
      ],
      "metadata": {
        "id": "q7o6sjljwuHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fetch the rewritten version of gpt149.py from [commit](https://github.com/Kelvinthedrugger/cs149gpt-copy/commit/0c8b8d78b3e26632183ff138281cba29db679a37)"
      ],
      "metadata": {
        "id": "EqZbo0XZv5rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the gpt149.py file & use the rewritten one instead\n",
        "!rm gpt149.py\n",
        "url =  \"https://raw.githubusercontent.com/Kelvinthedrugger/cs149gpt-copy/refs/heads/shorter_test/gpt149.py\"\n",
        "!wget {url}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG0klI34np0i",
        "outputId": "6296d600-0ad9-4c22-8f1b-32f7540ed90e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'gpt149.py': No such file or directory\n",
            "--2024-11-02 12:02:41--  https://raw.githubusercontent.com/Kelvinthedrugger/cs149gpt-copy/refs/heads/shorter_test/gpt149.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8664 (8.5K) [text/plain]\n",
            "Saving to: ‘gpt149.py’\n",
            "\n",
            "gpt149.py           100%[===================>]   8.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-02 12:02:41 (76.3 MB/s) - ‘gpt149.py’ saved [8664/8664]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8khllj_CnpWc",
        "outputId": "9a0a51d6-1cec-4db8-b27d-a6bcbc9a4dd9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 12:02:58 13327:13327 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 12:02:58 13327:13327 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 12:02:58 13327:13327 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Pytorch Execution Time: 0.03386688232421875 \n",
            "\n",
            "Manual Execution Time:  0.33853936195373535 \n",
            "\n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                    aten::empty         0.01%      40.000us         0.01%      40.000us      20.000us       1.00 Mb       1.00 Mb             2  \n",
            "                    aten::clone         0.03%      95.000us         0.30%       1.017ms     508.500us       1.00 Mb           0 b             2  \n",
            "                model_inference         0.02%      68.000us       100.00%     338.582ms     338.582ms     512.00 Kb           0 b             1  \n",
            "    REFERENCE - Naive Attention        99.58%     337.149ms        99.98%     338.514ms     338.514ms     512.00 Kb      -1.00 Mb             1  \n",
            "                    aten::zeros         0.01%      26.000us         0.07%     243.000us     243.000us     512.00 Kb           0 b             1  \n",
            "                  aten::flatten         0.02%      77.000us         0.19%     632.000us     126.400us     512.00 Kb           0 b             5  \n",
            "               aten::empty_like         0.01%      22.000us         0.01%      45.000us      45.000us     512.00 Kb           0 b             1  \n",
            "            aten::empty_strided         0.01%      47.000us         0.01%      47.000us      47.000us     512.00 Kb     512.00 Kb             1  \n",
            "                    aten::zero_         0.01%      46.000us         0.06%     200.000us     200.000us           0 b           0 b             1  \n",
            "                    aten::fill_         0.05%     154.000us         0.05%     154.000us     154.000us           0 b           0 b             1  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 338.582ms\n",
            "\n",
            "REFERENCE - Naive Attention statistics\n",
            "cpu time:  338.514ms\n",
            "mem usage:  524288 bytes\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 12:03:07 13327:13327 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 12:03:07 13327:13327 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 12:03:07 13327:13327 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Pytorch Execution Time: 0.033173322677612305 \n",
            "\n",
            "Manual Execution Time:  0.31943583488464355 \n",
            "\n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  aten::empty         0.01%      25.000us         0.01%      25.000us      12.500us       1.00 Mb       1.00 Mb             2  \n",
            "                  aten::clone         0.01%      46.000us         0.20%     639.000us     319.500us       1.00 Mb           0 b             2  \n",
            "              model_inference         0.02%      67.000us       100.00%     319.474ms     319.474ms     512.00 Kb           0 b             1  \n",
            "    STUDENT - Naive Attention        99.70%     318.514ms        99.98%     319.407ms     319.407ms     512.00 Kb      -1.00 Mb             1  \n",
            "                  aten::zeros         0.01%      21.000us         0.06%     185.000us     185.000us     512.00 Kb           0 b             1  \n",
            "                aten::flatten         0.01%      42.000us         0.14%     446.000us      89.200us     512.00 Kb           0 b             5  \n",
            "             aten::empty_like         0.00%       6.000us         0.00%      14.000us      14.000us     512.00 Kb           0 b             1  \n",
            "          aten::empty_strided         0.00%      15.000us         0.00%      15.000us      15.000us     512.00 Kb     512.00 Kb             1  \n",
            "                  aten::zero_         0.00%      11.000us         0.05%     147.000us     147.000us           0 b           0 b             1  \n",
            "                  aten::fill_         0.04%     136.000us         0.04%     136.000us     136.000us           0 b           0 b             1  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 319.474ms\n",
            "\n",
            "STUDENT - Naive Attention statistics\n",
            "cpu time:  319.407ms\n",
            "mem usage:  524288 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYQN9qxxskGW",
        "outputId": "98d86e50-d4d5-4389-9897-634d7fc3da13"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 12:03:16 13429:13429 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 12:03:16 13429:13429 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 12:03:16 13429:13429 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Pytorch Execution Time: 0.031322479248046875 \n",
            "\n",
            "Manual Execution Time:  0.27274656295776367 \n",
            "\n",
            "-----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                              aten::empty         0.01%      40.000us         0.01%      40.000us      20.000us       1.00 Mb       1.00 Mb             2  \n",
            "                              aten::clone         0.03%      91.000us         0.39%       1.066ms     533.000us       1.00 Mb           0 b             2  \n",
            "                          model_inference         0.03%      79.000us       100.00%     272.794ms     272.794ms     512.00 Kb           0 b             1  \n",
            "    REFERENCE - Blocked Unfused Attention        99.47%     271.340ms        99.97%     272.715ms     272.715ms     512.00 Kb      -1.00 Mb             1  \n",
            "                              aten::zeros         0.01%      32.000us         0.07%     203.000us     203.000us     512.00 Kb           0 b             1  \n",
            "                            aten::flatten         0.03%      74.000us         0.25%     687.000us     137.400us     512.00 Kb           0 b             5  \n",
            "                         aten::empty_like         0.01%      16.000us         0.01%      37.000us      37.000us     512.00 Kb           0 b             1  \n",
            "                      aten::empty_strided         0.02%      48.000us         0.02%      48.000us      48.000us     512.00 Kb     512.00 Kb             1  \n",
            "                              aten::zero_         0.01%      26.000us         0.06%     152.000us     152.000us           0 b           0 b             1  \n",
            "                              aten::fill_         0.05%     126.000us         0.05%     126.000us     126.000us           0 b           0 b             1  \n",
            "-----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 272.794ms\n",
            "\n",
            "REFERENCE - Blocked Unfused Attention statistics\n",
            "cpu time:  272.715ms\n",
            "mem usage:  524288 bytes\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 12:03:24 13429:13429 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 12:03:24 13429:13429 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 12:03:24 13429:13429 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Pytorch Execution Time: 0.030348539352416992 \n",
            "\n",
            "Manual Execution Time:  0.6066410541534424 \n",
            "\n",
            "---------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                            aten::empty         0.00%      24.000us         0.00%      24.000us      12.000us       1.00 Mb       1.00 Mb             2  \n",
            "                            aten::clone         0.01%      47.000us         0.10%     604.000us     302.000us       1.00 Mb           0 b             2  \n",
            "                        model_inference         0.01%      62.000us       100.00%     606.676ms     606.676ms     512.00 Kb           0 b             1  \n",
            "    STUDENT - Blocked Unfused Attention        99.85%     605.736ms        99.99%     606.614ms     606.614ms     512.00 Kb      -1.00 Mb             1  \n",
            "                            aten::zeros         0.00%      24.000us         0.03%     198.000us     198.000us     512.00 Kb           0 b             1  \n",
            "                          aten::flatten         0.01%      47.000us         0.07%     411.000us      82.200us     512.00 Kb           0 b             5  \n",
            "                       aten::empty_like         0.00%       6.000us         0.00%      14.000us      14.000us     512.00 Kb           0 b             1  \n",
            "                    aten::empty_strided         0.00%      15.000us         0.00%      15.000us      15.000us     512.00 Kb     512.00 Kb             1  \n",
            "                            aten::zero_         0.00%      12.000us         0.03%     158.000us     158.000us           0 b           0 b             1  \n",
            "                            aten::fill_         0.02%     146.000us         0.02%     146.000us     146.000us           0 b           0 b             1  \n",
            "---------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 606.676ms\n",
            "\n",
            "STUDENT - Blocked Unfused Attention statistics\n",
            "cpu time:  606.614ms\n",
            "mem usage:  524288 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D9jnEIGtUBT",
        "outputId": "95308e66-12f5-48c2-c8fd-ecd16d00409b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 12:03:34 13523:13523 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 12:03:34 13523:13523 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 12:03:34 13523:13523 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Pytorch Execution Time: 0.03096461296081543 \n",
            "\n",
            "Manual Execution Time:  0.24549531936645508 \n",
            "\n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                    aten::empty         0.02%      38.000us         0.02%      38.000us      19.000us       1.00 Mb       1.00 Mb             2  \n",
            "                    aten::clone         0.03%      85.000us         0.41%       1.017ms     508.500us       1.00 Mb           0 b             2  \n",
            "                model_inference         0.03%      62.000us       100.00%     245.533ms     245.533ms     512.00 Kb           0 b             1  \n",
            "    REFERENCE - Fused Attention        97.71%     239.919ms        99.97%     245.471ms     245.471ms     512.00 Kb      -1.00 Mb             1  \n",
            "                    aten::zeros         0.01%      32.000us         0.11%     260.000us     260.000us     512.00 Kb           0 b             1  \n",
            "                  aten::flatten         0.48%       1.177ms         0.98%       2.402ms       4.655us     512.00 Kb           0 b           516  \n",
            "               aten::empty_like         0.01%      16.000us         0.01%      35.000us      35.000us     512.00 Kb           0 b             1  \n",
            "            aten::empty_strided         0.01%      36.000us         0.01%      36.000us      36.000us     512.00 Kb     512.00 Kb             1  \n",
            "                    aten::zero_         0.02%      43.000us         0.09%     209.000us     209.000us           0 b           0 b             1  \n",
            "                    aten::fill_         0.07%     166.000us         0.07%     166.000us     166.000us           0 b           0 b             1  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 245.533ms\n",
            "\n",
            "REFERENCE - Fused Attention statistics\n",
            "cpu time:  245.471ms\n",
            "mem usage:  524288 bytes\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 12:03:43 13523:13523 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 12:03:44 13523:13523 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 12:03:44 13523:13523 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Pytorch Execution Time: 0.07127690315246582 \n",
            "\n",
            "Manual Execution Time:  0.5227861404418945 \n",
            "\n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  aten::empty         0.01%      32.000us         0.01%      32.000us      10.667us       1.00 Mb       1.00 Mb             3  \n",
            "                  aten::clone         0.01%      41.000us         0.72%       3.779ms       1.889ms       1.00 Mb           0 b             2  \n",
            "                  aten::zeros         0.01%      39.000us         0.28%       1.451ms     725.500us     516.00 Kb           0 b             2  \n",
            "              model_inference         0.02%      87.000us       100.00%     522.838ms     522.838ms     512.00 Kb           0 b             1  \n",
            "    STUDENT - Fused Attention        86.18%     450.582ms        99.98%     522.751ms     522.751ms     512.00 Kb      -1.00 Mb             1  \n",
            "                aten::flatten         0.23%       1.219ms         4.41%      23.062ms      44.694us     512.00 Kb           0 b           516  \n",
            "             aten::empty_like         0.00%       7.000us         0.00%      12.000us      12.000us     512.00 Kb           0 b             1  \n",
            "          aten::empty_strided         0.00%      17.000us         0.00%      17.000us      17.000us     512.00 Kb     512.00 Kb             1  \n",
            "                  aten::zero_         0.00%      18.000us         0.26%       1.385ms     692.500us           0 b           0 b             2  \n",
            "                  aten::fill_         0.26%       1.367ms         0.26%       1.367ms       1.367ms           0 b           0 b             1  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 522.838ms\n",
            "\n",
            "STUDENT - Fused Attention statistics\n",
            "cpu time:  522.751ms\n",
            "mem usage:  524288 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VUvVxS-tZx9",
        "outputId": "472d39ca-fc15-466f-ea42-efb78d384aa3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 12:03:54 13633:13633 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 12:03:54 13633:13633 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 12:03:54 13633:13633 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Pytorch Execution Time: 0.03150534629821777 \n",
            "\n",
            "Manual Execution Time:  0.6318767070770264 \n",
            "\n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                    aten::zeros         0.01%      47.000us         0.58%       3.645ms       1.215ms       8.50 Mb           0 b             3  \n",
            "                    aten::empty         0.01%      66.000us         0.01%      66.000us      22.000us       8.50 Mb       8.50 Mb             3  \n",
            "                model_inference         0.01%      65.000us       100.00%     631.915ms     631.915ms     512.00 Kb           0 b             1  \n",
            "    REFERENCE - Flash Attention        97.53%     616.285ms        99.99%     631.850ms     631.850ms     512.00 Kb      -8.00 Mb             1  \n",
            "                    aten::zero_         0.22%       1.414ms         2.45%      15.452ms      43.042us           0 b           0 b           359  \n",
            "                    aten::fill_         2.22%      14.038ms         2.22%      14.038ms     107.160us           0 b           0 b           131  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 631.915ms\n",
            "\n",
            "REFERENCE - Flash Attention statistics\n",
            "cpu time:  631.85ms\n",
            "mem usage:  524288 bytes\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-11-02 12:04:02 13633:13633 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-11-02 12:04:02 13633:13633 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-11-02 12:04:02 13633:13633 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Pytorch Execution Time: 0.031051158905029297 \n",
            "\n",
            "Manual Execution Time:  0.33042263984680176 \n",
            "\n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  aten::empty         0.01%      29.000us         0.01%      29.000us      14.500us       1.00 Mb       1.00 Mb             2  \n",
            "                  aten::clone         0.03%      87.000us         0.22%     722.000us     361.000us       1.00 Mb           0 b             2  \n",
            "              model_inference         0.03%      87.000us       100.00%     330.473ms     330.473ms     512.00 Kb           0 b             1  \n",
            "    STUDENT - Flash Attention        99.64%     329.296ms        99.97%     330.386ms     330.386ms     512.00 Kb      -1.00 Mb             1  \n",
            "                  aten::zeros         0.01%      26.000us         0.07%     226.000us     226.000us     512.00 Kb           0 b             1  \n",
            "                aten::flatten         0.03%      92.000us         0.18%     586.000us      39.067us     512.00 Kb           0 b            15  \n",
            "             aten::empty_like         0.00%      16.000us         0.01%      26.000us      26.000us     512.00 Kb           0 b             1  \n",
            "          aten::empty_strided         0.00%      15.000us         0.00%      15.000us      15.000us     512.00 Kb     512.00 Kb             1  \n",
            "                  aten::zero_         0.00%      15.000us         0.05%     181.000us     181.000us           0 b           0 b             1  \n",
            "                  aten::fill_         0.05%     166.000us         0.05%     166.000us     166.000us           0 b           0 b             1  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 330.473ms\n",
            "\n",
            "STUDENT - Flash Attention statistics\n",
            "cpu time:  330.386ms\n",
            "mem usage:  524288 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yantf050t16P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}