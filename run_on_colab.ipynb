{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### use openmp branch so that\n",
        "\n",
        "result tested on this [commit](https://github.com/Kelvinthedrugger/cs149gpt-copy/tree/a2ad3f6764c5ba1c9210a9e3353380f143f7c544)\n",
        "\n",
        "avx is used (google uses x64 cpu for colab)\n",
        "\n",
        "openmp is used\n",
        "\n",
        "you only need cpu to perform this test"
      ],
      "metadata": {
        "id": "6g7jMKZCcsOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --single-branch --branch openmp https://github.com/Kelvinthedrugger/cs149gpt-copy.git"
      ],
      "metadata": {
        "id": "pNkMSe8va90Q",
        "outputId": "1f75cd83-8e84-4ff4-f1e6-1a17d296c8c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cs149gpt-copy'...\n",
            "remote: Enumerating objects: 182, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 182 (delta 71), reused 65 (delta 61), pack-reused 100 (from 1)\u001b[K\n",
            "Receiving objects: 100% (182/182), 11.98 MiB | 8.21 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd cs149gpt-copy"
      ],
      "metadata": {
        "id": "FkRldZZmbAYo",
        "outputId": "eb37cd1f-2883-4923-b33e-5340b291cbd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cs149gpt-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch"
      ],
      "metadata": {
        "id": "h39RFwf9bDmx",
        "outputId": "4b25cbd9-6faa-4b43-970a-53759f36eaa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mopenmp\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat requirements.txt"
      ],
      "metadata": {
        "id": "KHVAIKDtbEci",
        "outputId": "c60d8b81-0980-46af-e92f-d6330ea014d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch==2.1.2 --index-url https://download.pytorch.org/whl/cpu # to run module_ref.so on x86-64\n",
            "Ninja\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6jY2xpSxbHhY",
        "outputId": "6ac445e0-5e75-4ace-fe63-d78901a9b7c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch"
      ],
      "metadata": {
        "id": "OykpLNybccfL",
        "outputId": "32b752ff-a548-4ff7-a890-6a4d16e8a0f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mmine\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part1"
      ],
      "metadata": {
        "id": "DV97JMsCbK1C",
        "outputId": "a37d2984-f115-4610-b130-d7918c2aa629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "Running Part 1 Test: Naive Unfused Attention\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-10-19 15:53:26 4932:4932 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-10-19 15:53:26 4932:4932 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-10-19 15:53:26 4932:4932 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.3468148708343506 \n",
            "\n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                    aten::empty         0.03%     113.000us         0.03%     113.000us      37.667us       5.00 Mb       5.00 Mb             3  \n",
            "    REFERENCE - NAIVE ATTENTION        98.90%     343.071ms        99.96%     346.762ms     346.762ms       4.50 Mb      -1.00 Mb             1  \n",
            "                    aten::zeros         0.02%      64.000us         0.61%       2.133ms       1.067ms       4.50 Mb           0 b             2  \n",
            "                    aten::clone         0.04%     122.000us         0.41%       1.408ms     704.000us       1.00 Mb           0 b             2  \n",
            "                model_inference         0.04%     125.000us       100.00%     346.887ms     346.887ms     512.00 Kb      -4.00 Mb             1  \n",
            "                  aten::flatten         0.03%      90.000us         0.25%     857.000us     171.400us     512.00 Kb           0 b             5  \n",
            "               aten::empty_like         0.00%      17.000us         0.01%      36.000us      36.000us     512.00 Kb           0 b             1  \n",
            "            aten::empty_strided         0.02%      58.000us         0.02%      58.000us      58.000us     512.00 Kb     512.00 Kb             1  \n",
            "                    aten::zero_         0.02%      68.000us         0.57%       1.975ms     987.500us           0 b           0 b             2  \n",
            "                    aten::fill_         0.55%       1.907ms         0.55%       1.907ms     953.500us           0 b           0 b             2  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 346.887ms\n",
            "\n",
            "REFERENCE - NAIVE ATTENTION statistics\n",
            "cpu time:  346.762ms\n",
            "mem usage:  4718592 bytes\n",
            "ERROR:2024-10-19 15:53:26 4932:4932 CudaDeviceProperties.cpp:27] cudaGetDeviceCount failed with code 35\n",
            "dump REFERENCE_-_NAIVE_ATTENTION.json succeeded\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-10-19 15:53:34 4932:4932 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-10-19 15:53:34 4932:4932 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-10-19 15:53:34 4932:4932 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.3297460079193115 \n",
            "\n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  aten::empty         0.01%      43.000us         0.01%      43.000us      14.333us       5.00 Mb       5.00 Mb             3  \n",
            "    STUDENT - NAIVE ATTENTION        99.37%     327.707ms        99.97%     329.694ms     329.694ms       4.50 Mb      -1.00 Mb             1  \n",
            "                  aten::zeros         0.02%      51.000us         0.32%       1.065ms     532.500us       4.50 Mb           0 b             2  \n",
            "                  aten::clone         0.02%      58.000us         0.25%     826.000us     413.000us       1.00 Mb           0 b             2  \n",
            "              model_inference         0.03%      97.000us       100.00%     329.791ms     329.791ms     512.00 Kb      -4.00 Mb             1  \n",
            "                aten::flatten         0.02%      57.000us         0.18%     578.000us     115.600us     512.00 Kb           0 b             5  \n",
            "             aten::empty_like         0.00%       9.000us         0.01%      17.000us      17.000us     512.00 Kb           0 b             1  \n",
            "          aten::empty_strided         0.01%      20.000us         0.01%      20.000us      20.000us     512.00 Kb     512.00 Kb             1  \n",
            "                  aten::zero_         0.01%      25.000us         0.30%     979.000us     489.500us           0 b           0 b             2  \n",
            "                  aten::fill_         0.29%     954.000us         0.29%     954.000us     477.000us           0 b           0 b             2  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 329.791ms\n",
            "\n",
            "STUDENT - NAIVE ATTENTION statistics\n",
            "cpu time:  329.694ms\n",
            "mem usage:  4718592 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part2"
      ],
      "metadata": {
        "id": "JFoRcJnzbU75",
        "outputId": "598e2e55-1388-4626-cdfc-6e02c99bcde2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "Running Part 2 Test: Unfused Attention with Blocked Matmul\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-10-19 15:55:22 5500:5500 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-10-19 15:55:22 5500:5500 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-10-19 15:55:22 5500:5500 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.2830829620361328 \n",
            "\n",
            "------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                     aten::empty         0.03%      82.000us         0.03%      82.000us      27.333us       5.00 Mb       5.00 Mb             3  \n",
            "    REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX        98.92%     280.095ms        99.96%     283.031ms     283.031ms       4.50 Mb      -1.00 Mb             1  \n",
            "                                     aten::zeros         0.02%      54.000us         0.60%       1.710ms     855.000us       4.50 Mb           0 b             2  \n",
            "                                     aten::clone         0.03%      91.000us         0.39%       1.102ms     551.000us       1.00 Mb           0 b             2  \n",
            "                                 model_inference         0.04%     121.000us       100.00%     283.152ms     283.152ms     512.00 Kb      -4.00 Mb             1  \n",
            "                                   aten::flatten         0.03%      87.000us         0.26%     730.000us     146.000us     512.00 Kb           0 b             5  \n",
            "                                aten::empty_like         0.00%      14.000us         0.01%      32.000us      32.000us     512.00 Kb           0 b             1  \n",
            "                             aten::empty_strided         0.02%      44.000us         0.02%      44.000us      44.000us     512.00 Kb     512.00 Kb             1  \n",
            "                                     aten::zero_         0.02%      53.000us         0.56%       1.592ms     796.000us           0 b           0 b             2  \n",
            "                                     aten::fill_         0.54%       1.539ms         0.54%       1.539ms     769.500us           0 b           0 b             2  \n",
            "------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 283.152ms\n",
            "\n",
            "REFERENCE - BLOCKED MATMUL + UNFUSED SOFTMAX statistics\n",
            "cpu time:  283.031ms\n",
            "mem usage:  4718592 bytes\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-10-19 15:55:31 5500:5500 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-10-19 15:55:32 5500:5500 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-10-19 15:55:32 5500:5500 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.5760061740875244 \n",
            "\n",
            "----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                   aten::empty         0.01%      69.000us         0.01%      69.000us      23.000us       5.00 Mb       5.00 Mb             3  \n",
            "    STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX        99.48%     573.018ms        99.99%     575.963ms     575.963ms       4.50 Mb      -1.00 Mb             1  \n",
            "                                   aten::zeros         0.01%      63.000us         0.38%       2.197ms       1.099ms       4.50 Mb           0 b             2  \n",
            "                                   aten::clone         0.01%      47.000us         0.12%     674.000us     337.000us       1.00 Mb           0 b             2  \n",
            "                               model_inference         0.01%      78.000us       100.00%     576.041ms     576.041ms     512.00 Kb      -4.00 Mb             1  \n",
            "                                 aten::flatten         0.01%      43.000us         0.08%     468.000us      93.600us     512.00 Kb           0 b             5  \n",
            "                              aten::empty_like         0.00%       8.000us         0.00%      16.000us      16.000us     512.00 Kb           0 b             1  \n",
            "                           aten::empty_strided         0.00%      16.000us         0.00%      16.000us      16.000us     512.00 Kb     512.00 Kb             1  \n",
            "                                   aten::zero_         0.00%      23.000us         0.36%       2.073ms       1.036ms           0 b           0 b             2  \n",
            "                                   aten::fill_         0.36%       2.050ms         0.36%       2.050ms       1.025ms           0 b           0 b             2  \n",
            "----------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 576.041ms\n",
            "\n",
            "STUDENT - BLOCKED MATMUL + UNFUSED SOFTMAX statistics\n",
            "cpu time:  575.963ms\n",
            "mem usage:  4718592 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 gpt149.py part3"
      ],
      "metadata": {
        "id": "ZQsqEL3H7jEZ",
        "outputId": "e6cbdef0-52f4-4223-d5c7-8e5618002f22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "Running Part 3 Test: Fused Attention\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-10-20 03:28:02 4753:4753 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-10-20 03:28:02 4753:4753 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-10-20 03:28:02 4753:4753 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.2786693572998047 \n",
            "\n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                    aten::empty         0.01%      28.000us         0.01%      28.000us       9.333us       1.03 Mb       1.03 Mb             3  \n",
            "                    aten::clone         0.03%      88.000us         0.31%     854.000us     427.000us       1.00 Mb           0 b             2  \n",
            "    REFERENCE - FUSED ATTENTION        94.23%     262.629ms        99.97%     278.633ms     278.633ms     544.00 Kb      -1.00 Mb             1  \n",
            "                    aten::zeros         0.02%      46.000us         0.11%     303.000us     151.500us     544.00 Kb           0 b             2  \n",
            "                model_inference         0.03%      78.000us       100.00%     278.711ms     278.711ms     512.00 Kb     -32.00 Kb             1  \n",
            "                  aten::flatten         0.46%       1.278ms         0.85%       2.373ms       4.599us     512.00 Kb           0 b           516  \n",
            "               aten::empty_like         0.01%      14.000us         0.01%      21.000us      21.000us     512.00 Kb           0 b             1  \n",
            "            aten::empty_strided         0.01%      39.000us         0.01%      39.000us      39.000us     512.00 Kb     512.00 Kb             1  \n",
            "                    aten::zero_         0.02%      43.000us         0.08%     236.000us     118.000us           0 b           0 b             2  \n",
            "                    aten::fill_         0.07%     193.000us         0.07%     193.000us     193.000us           0 b           0 b             1  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 278.711ms\n",
            "\n",
            "REFERENCE - FUSED ATTENTION statistics\n",
            "cpu time:  278.633ms\n",
            "mem usage:  557056 bytes\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-10-20 03:28:11 4753:4753 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-10-20 03:28:11 4753:4753 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-10-20 03:28:11 4753:4753 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.25700807571411133 \n",
            "\n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  aten::empty         0.01%      31.000us         0.01%      31.000us       7.750us       1.04 Mb       1.04 Mb             4  \n",
            "                  aten::clone         0.01%      38.000us         0.23%     594.000us     297.000us       1.00 Mb           0 b             2  \n",
            "                  aten::zeros         0.01%      34.000us         0.09%     229.000us      76.333us     548.00 Kb           0 b             3  \n",
            "    STUDENT - FUSED ATTENTION        90.56%     232.783ms        99.97%     256.972ms     256.972ms     544.00 Kb      -1.00 Mb             1  \n",
            "              model_inference         0.03%      83.000us       100.00%     257.055ms     257.055ms     512.00 Kb     -32.00 Kb             1  \n",
            "                aten::flatten         3.57%       9.174ms         3.94%      10.126ms      19.624us     512.00 Kb           0 b           516  \n",
            "             aten::empty_like         0.00%       7.000us         0.01%      14.000us      14.000us     512.00 Kb           0 b             1  \n",
            "          aten::empty_strided         0.01%      15.000us         0.01%      15.000us      15.000us     512.00 Kb     512.00 Kb             1  \n",
            "                  aten::zero_         0.01%      20.000us         0.07%     171.000us      57.000us           0 b           0 b             3  \n",
            "                  aten::fill_         0.06%     151.000us         0.06%     151.000us     151.000us           0 b           0 b             1  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 257.055ms\n",
            "\n",
            "STUDENT - FUSED ATTENTION statistics\n",
            "cpu time:  256.972ms\n",
            "mem usage:  557056 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set Bc = Br = 16 to give a better performance\n",
        "!python3 gpt149.py part4 -bc 16 -br 16"
      ],
      "metadata": {
        "id": "7Fi3oJTRbXzH",
        "outputId": "d0b25218-b6f7-4289-9709-4194b6dd8b96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "Compiling code into a PyTorch module...\n",
            "\n",
            "\n",
            "Running Part 4 Test: Flash Attention\n",
            "\n",
            "-----RUNNING REFERENCE IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-10-19 15:50:21 4159:4159 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-10-19 15:50:22 4159:4159 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-10-19 15:50:22 4159:4159 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.734555721282959 \n",
            "\n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                    aten::zeros         0.01%      99.000us         0.62%       4.519ms     322.786us       8.52 Mb       2.00 Kb            14  \n",
            "                    aten::empty         0.02%     112.000us         0.02%     112.000us       8.000us       8.51 Mb       8.51 Mb            14  \n",
            "                model_inference         0.05%     344.000us       100.00%     734.614ms     734.614ms     512.00 Kb     -16.19 Kb             1  \n",
            "    REFERENCE - FLASH ATTENTION        99.16%     728.466ms        99.94%     734.155ms     734.155ms     512.00 Kb      -8.00 Mb             1  \n",
            "                    aten::zero_         0.18%       1.355ms         0.76%       5.593ms       0.068us           0 b           0 b         82438  \n",
            "                    aten::fill_         0.58%       4.238ms         0.58%       4.238ms       1.413ms           0 b           0 b             3  \n",
            "-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 734.614ms\n",
            "\n",
            "REFERENCE - FLASH ATTENTION statistics\n",
            "cpu time:  734.155ms\n",
            "mem usage:  524288 bytes\n",
            "ERROR:2024-10-19 15:50:35 4159:4159 CudaDeviceProperties.cpp:27] cudaGetDeviceCount failed with code 35\n",
            "dump REFERENCE_-_FLASH_ATTENTION.json succeeded\n",
            "-----RUNNING STUDENT IMPLEMENTATION-----\n",
            "\n",
            "STAGE:2024-10-19 15:50:45 4159:4159 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
            "STAGE:2024-10-19 15:50:45 4159:4159 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
            "STAGE:2024-10-19 15:50:45 4159:4159 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
            "manual attention == pytorch attention True\n",
            "Manual Execution Time:  0.29160261154174805 \n",
            "\n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  aten::empty         0.01%      40.000us         0.01%      40.000us       3.077us       1.01 Mb       1.01 Mb            13  \n",
            "                  aten::clone         0.03%      84.000us         0.23%     661.000us     330.500us       1.00 Mb           0 b             2  \n",
            "                  aten::zeros         0.02%      58.000us         0.09%     263.000us      21.917us     528.19 Kb       2.00 Kb            12  \n",
            "              model_inference         0.06%     188.000us       100.00%     291.646ms     291.646ms     512.00 Kb     -16.25 Kb             1  \n",
            "    STUDENT - FLASH ATTENTION        99.58%     290.429ms        99.91%     291.373ms     291.373ms     512.00 Kb      -1.00 Mb             1  \n",
            "                aten::flatten         0.03%      73.000us         0.16%     479.000us      31.933us     512.00 Kb           0 b            15  \n",
            "             aten::empty_like         0.00%      13.000us         0.01%      21.000us      21.000us     512.00 Kb           0 b             1  \n",
            "          aten::empty_strided         0.01%      18.000us         0.01%      18.000us      18.000us     512.00 Kb     512.00 Kb             1  \n",
            "                  aten::zero_         0.00%      14.000us         0.06%     173.000us      14.417us          64 b          64 b            12  \n",
            "                  aten::fill_         0.05%     159.000us         0.05%     159.000us     159.000us           0 b           0 b             1  \n",
            "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 291.646ms\n",
            "\n",
            "STUDENT - FLASH ATTENTION statistics\n",
            "cpu time:  291.373ms\n",
            "mem usage:  524288 bytes\n",
            "dump STUDENT_-_FLASH_ATTENTION.json succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgp-2HtRb0hX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}